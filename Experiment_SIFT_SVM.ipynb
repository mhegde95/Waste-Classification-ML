{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45391d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------done processing-------------------\n",
      "--------------done scaling-------------------\n",
      "--------------done pca-------------------\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# Define the path to your \"train\" folder\n",
    "train_folder_path = 'TRAIN'\n",
    "\n",
    "# Image resize dimensions\n",
    "resize_width = 50\n",
    "resize_height = 50\n",
    "\n",
    "# Maximum number of samples to load\n",
    "max_samples = 30000\n",
    "\n",
    "# Number of SIFT features to extract per image\n",
    "num_sift_features = 100  \n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Get the list of subfolders (class labels) in the \"train\" folder\n",
    "class_labels = os.listdir(train_folder_path)\n",
    "\n",
    "# Load and preprocess a random sample of images from each class\n",
    "for label in class_labels:\n",
    "    class_folder = os.path.join(train_folder_path, label)\n",
    "    image_files = [os.path.join(class_folder, filename) for filename in os.listdir(class_folder) if filename.endswith('.jpg')]\n",
    "    \n",
    "    # Shuffle the list of image files\n",
    "    random.shuffle(image_files)\n",
    "    \n",
    "    for image_file in image_files[:max_samples // len(class_labels)]:\n",
    "        image = cv2.imread(image_file)\n",
    "        image = cv2.resize(image, (resize_width, resize_height))\n",
    "\n",
    "        # Feature extraction using SIFT\n",
    "        sift = cv2.SIFT_create()\n",
    "        keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "        \n",
    "        if descriptors is not None:\n",
    "            # Ensure a consistent number of SIFT features for each image\n",
    "            if descriptors.shape[0] < num_sift_features:\n",
    "                # If fewer features are detected, add zero rows\n",
    "                zero_rows = np.zeros((num_sift_features - descriptors.shape[0], descriptors.shape[1]))\n",
    "                descriptors = np.vstack((descriptors, zero_rows))\n",
    "            elif descriptors.shape[0] > num_sift_features:\n",
    "                # If more features are detected, truncate to the desired number\n",
    "                descriptors = descriptors[:num_sift_features, :]\n",
    "\n",
    "            X.extend(descriptors)\n",
    "            y.extend([class_labels.index(label)] * num_sift_features)  # Assign labels based on class folders\n",
    "\n",
    "print(\"--------------done processing-------------------\")\n",
    "\n",
    "## Store the values of X and Y in nump\n",
    "X = np.array(X) if len(X) > 0 else np.empty((0, 128))  # Assuming SIFT descriptors have 128 dimensions\n",
    "y = np.array(y)\n",
    "\n",
    "# Normalization of features using scaler\n",
    "# Reference : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "scaler_norm = preprocessing.StandardScaler()\n",
    "X = scaler_norm.fit_transform(X)\n",
    "\n",
    "\n",
    "# Reducing the dimension of features using PCA which will help us to extract latent features and reduce dimensions\n",
    "\n",
    "# Define variables required for PCA\n",
    "total_samples, total_features = X.shape\n",
    "number_of_components = min(total_samples, total_features) \n",
    "\n",
    "# Define PCA (Incremental PCA)\n",
    "incremental_pca = IncrementalPCA(n_components=number_of_components)\n",
    "\n",
    "# Process in batches otherwise the run time will be forever\n",
    "\n",
    "# Reference - https://stackoverflow.com/questions/66716370/batch-size-and-training-time\n",
    "batch_size = total_samples  \n",
    "for i in range(0, X.shape[0], batch_size):\n",
    "    X_batch_pca = X[i:i + batch_size]\n",
    "    incremental_pca.partial_fit(X_batch_pca)\n",
    "\n",
    "# Reference : https://scikit-learn.org/stable/auto_examples/decomposition/plot_incremental_pca.html\n",
    "X_pca = incremental_pca.transform(X)  \n",
    "\n",
    "\n",
    "print(\"--------------done pca-------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acde3046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Feature_0  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  \\\n",
      "435200   13.831749  -7.929849  -0.058160  -2.698314  -0.868951  -0.644826   \n",
      "539910   -3.402186  -0.092564  -0.056431   0.000413   0.035653   0.071824   \n",
      "1369603   8.924907  -4.055584   5.103814  -4.658314  -0.702178  -2.208232   \n",
      "1455361  -3.402186  -0.092564  -0.056431   0.000413   0.035653   0.071824   \n",
      "2014138  -3.402186  -0.092564  -0.056431   0.000413   0.035653   0.071824   \n",
      "...            ...        ...        ...        ...        ...        ...   \n",
      "2354380  -3.402186  -0.092564  -0.056431   0.000413   0.035653   0.071824   \n",
      "2456894  -3.402186  -0.092564  -0.056431   0.000413   0.035653   0.071824   \n",
      "11810    16.448621  -2.116250 -10.686191  -0.677939  -5.790890   4.822818   \n",
      "918957   -3.402186  -0.092564  -0.056431   0.000413   0.035653   0.071824   \n",
      "1938778  -3.402186  -0.092564  -0.056431   0.000413   0.035653   0.071824   \n",
      "\n",
      "         Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_119  \\\n",
      "435200   -0.161915  -0.623212  -1.612141   5.235828  ...    -0.527945   \n",
      "539910    0.036136  -0.000185  -0.026646  -0.001294  ...     0.000055   \n",
      "1369603   2.618599  -2.509264   4.412319   3.132648  ...     0.149813   \n",
      "1455361   0.036136  -0.000185  -0.026646  -0.001294  ...     0.000055   \n",
      "2014138   0.036136  -0.000185  -0.026646  -0.001294  ...     0.000055   \n",
      "...            ...        ...        ...        ...  ...          ...   \n",
      "2354380   0.036136  -0.000185  -0.026646  -0.001294  ...     0.000055   \n",
      "2456894   0.036136  -0.000185  -0.026646  -0.001294  ...     0.000055   \n",
      "11810     5.294214  -0.524192   1.238391   1.406080  ...    -0.095095   \n",
      "918957    0.036136  -0.000185  -0.026646  -0.001294  ...     0.000055   \n",
      "1938778   0.036136  -0.000185  -0.026646  -0.001294  ...     0.000055   \n",
      "\n",
      "         Feature_120  Feature_121  Feature_122  Feature_123  Feature_124  \\\n",
      "435200      0.502591     0.147034     0.039035     0.367129     0.658095   \n",
      "539910      0.000069    -0.001341    -0.000005     0.000016    -0.000084   \n",
      "1369603     0.136587    -0.533803     0.510742    -0.082597    -0.298689   \n",
      "1455361     0.000069    -0.001341    -0.000005     0.000016    -0.000084   \n",
      "2014138     0.000069    -0.001341    -0.000005     0.000016    -0.000084   \n",
      "...              ...          ...          ...          ...          ...   \n",
      "2354380     0.000069    -0.001341    -0.000005     0.000016    -0.000084   \n",
      "2456894     0.000069    -0.001341    -0.000005     0.000016    -0.000084   \n",
      "11810      -0.611381    -0.919953     0.282971    -0.457967    -0.196295   \n",
      "918957      0.000069    -0.001341    -0.000005     0.000016    -0.000084   \n",
      "1938778     0.000069    -0.001341    -0.000005     0.000016    -0.000084   \n",
      "\n",
      "          Feature_125  Feature_126  Feature_127  Label  \n",
      "435200   3.354262e-01    -0.089682     0.022779    0.0  \n",
      "539910  -5.473942e-07     0.000008    -0.000009    0.0  \n",
      "1369603 -1.105059e-01    -0.091843     0.141017    0.0  \n",
      "1455361 -5.473942e-07     0.000008    -0.000009    1.0  \n",
      "2014138 -5.473942e-07     0.000008    -0.000009    1.0  \n",
      "...               ...          ...          ...    ...  \n",
      "2354380 -5.473942e-07     0.000008    -0.000009    1.0  \n",
      "2456894 -5.473942e-07     0.000008    -0.000009    1.0  \n",
      "11810   -3.282939e-01    -0.125304     0.252878    0.0  \n",
      "918957  -5.473942e-07     0.000008    -0.000009    0.0  \n",
      "1938778 -5.473942e-07     0.000008    -0.000009    1.0  \n",
      "\n",
      "[10000 rows x 129 columns]\n",
      "--------------done SVM-------------------\n",
      "Test Accuracy: 55.80%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = np.column_stack((X_pca, y))\n",
    "column_names = [f\"Feature_{i}\" for i in range(X_pca.shape[1])] + [\"Label\"]\n",
    "df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "shuffled_df_3 = df.sample(frac=1, random_state=42)  # \"frac=1\" shuffles all rows\n",
    "\n",
    "# Display the DataFrame\n",
    "#Select the first 10 rows\n",
    "selected_df_6 = shuffled_df_3.head(10000)\n",
    "\n",
    "# Display the selected DataFrame\n",
    "print(selected_df_6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize an SVM classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = selected_df_6.iloc[:, :-1]  # All columns except the last one\n",
    "y = selected_df_6.iloc[:, -1]   # The last column\n",
    "\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize an SVM classifier\n",
    "clf = SVC(C=1.0, kernel='rbf', gamma='auto')\n",
    "\n",
    "# Train the SVM model on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"--------------done SVM-------------------\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy for this fold\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save the trained model and PCA to a single pickle file\n",
    "model_and_pca = {\n",
    "    'model': clf,\n",
    "    'pca': ipca,\n",
    "    'scaler': scaler\n",
    "}\n",
    "\n",
    "model_path = 'svm_model_pca.pkl'\n",
    "with open(model_path, 'wb') as model_file:\n",
    "    pickle.dump(model_and_pca, model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f840cfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 55.80%\n",
      "Sensitivity is 1.36%\n",
      "Precision is 46.15%\n",
      "F1-score is 2.64%\n",
      "Cohen - kappa statistic is 0.12%\n",
      "MCC is 0.47%\n",
      "Confusion Matrix: \n",
      " [[1104   14]\n",
      " [ 870   12]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.99      0.71      1118\n",
      "         1.0       0.46      0.01      0.03       882\n",
      "\n",
      "    accuracy                           0.56      2000\n",
      "   macro avg       0.51      0.50      0.37      2000\n",
      "weighted avg       0.52      0.56      0.41      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef\n",
    "# Calculate test accuracy of SVM model\n",
    "# Reference : https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "sensitivity = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1_score = f1_score(y_test, y_pred)\n",
    "cohen_kappa = cohen_kappa_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "      \n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f'Sensitivity is {sensitivity * 100:.2f}%')\n",
    "print(f'Precision is {precision * 100:.2f}%')\n",
    "print(f'F1-score is {f1_score * 100:.2f}%')\n",
    "print(f'Cohen - kappa statistic is {cohen_kappa * 100:.2f}%')\n",
    "print(f'MCC is {mcc * 100:.2f}%')\n",
    "print('Confusion Matrix: \\n', confusion_matrix(y_test,y_pred))\n",
    "print('Classification Report: \\n', classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f333a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
